{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdMcJq0B36pnTsiLYe2lUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikemaurrasse-hash/mikemAIM460/blob/main/Maurrasse_K_NN_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6WixJn4qnen"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-NN Housing Price Prediction"
      ],
      "metadata": {
        "id": "MvsBAp8brF1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-NN Housing Price Prediction\n",
        "**Student:** Mike Maurrasse  \n",
        "**Date:** 8/31/2025\n",
        "\n",
        "This notebook implements a pipeline for predicting California house prices using a custom k-Nearest Neighbors model.\n"
      ],
      "metadata": {
        "id": "umxJLOHWrmb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.width', 120)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n"
      ],
      "metadata": {
        "id": "oWxhA896r3Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Exploration"
      ],
      "metadata": {
        "id": "cMcD20jSr_F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cali = fetch_california_housing(as_frame=True)\n",
        "df = cali.frame.copy()\n",
        "df.columns = ['MedInc','HouseAge','AveRooms','AveBedrms','Population','AveOccup','Latitude','Longitude','MedHouseVal']\n",
        "target_col = 'MedHouseVal'\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "GvVyRshTsH8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA Write-up\n",
        "When I explored the target variable (median house value), I saw that the distribution was right-skewed. Most houses are in the lower-to-mid price range, and there are a few very expensive homes that show up as outliers. These can be flagged statistically, but I consider them realistic because California does have very high-value properties.\n",
        "\n",
        "I noticed that median income, latitude, and average rooms per household are the features most strongly tied to prices. Income has a clear positive relationship with home values, and geography stood out too — coastal and metro areas like San Francisco and Los Angeles drive higher prices. Latitude also reflects the north–south gradient across the state.\n",
        "\n",
        "Some challenges I’ll need to manage include skewed feature distributions (like income, rooms, population), the presence of outliers, and multicollinearity among room-related variables. To address this, scaling and careful feature engineering will be important for keeping the model stable and fair.\n",
        "\n"
      ],
      "metadata": {
        "id": "eEd-bB2wsW26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(df[target_col], bins=40)\n",
        "plt.title(\"Distribution of Median House Value\")\n",
        "plt.xlabel(\"MedHouseVal\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "corr = df.corr()\n",
        "plt.imshow(corr, cmap='coolwarm', interpolation='nearest')\n",
        "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
        "plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VWuW53QcsaVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers\n",
        "Both the IQR and Z-score methods flagged expensive homes and big population counts as outliers. If I dropped them, the dataset would shrink a lot and I’d also lose valid housing scenarios, like dense city blocks. For that reason, I decided **not to remove outliers**. Instead, I’ll leave them in, note their influence, and rely on scaling and robust models to manage their impact.\n",
        "\n"
      ],
      "metadata": {
        "id": "rvoebNWwskak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Example"
      ],
      "metadata": {
        "id": "xzHPziC4ulTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1, q3 = df[target_col].quantile([0.25, 0.75])\n",
        "iqr = q3 - q1\n",
        "lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
        "outliers = ((df[target_col] < lower) | (df[target_col] > upper)).sum()\n",
        "print(\"Potential outliers in target:\", outliers)\n"
      ],
      "metadata": {
        "id": "W7j2Kub4s4Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling Choice\n",
        "## Why I Picked StandardScaler\n",
        "I decided to use **StandardScaler** for preprocessing. My features are on very different scales — population can be in the thousands, while household size is just a few digits. Without scaling, distance metrics would overweight the larger numbers.\n",
        "\n",
        "I avoided MinMaxScaler because it compresses most values into a tiny range when outliers are present. StandardScaler centers the data and reduces the risk of one variable dominating the distance calculation.\n"
      ],
      "metadata": {
        "id": "gA5k98SjvUpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "FtKMq7AHvhFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['rooms_per_household'] = df['AveRooms'] / df['AveOccup']\n",
        "df['bedrooms_per_room'] = df['AveBedrms'] / df['AveRooms']\n",
        "df['population_per_household'] = df['Population'] / df['AveOccup']\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Krgfhm01vlls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom k-NN Implementation"
      ],
      "metadata": {
        "id": "asDDd9rOtUOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(point1, point2):\n",
        "    diff = point1 - point2\n",
        "    return float(np.sqrt(np.dot(diff, diff)))\n",
        "\n",
        "def manhattan_distance(point1, point2):\n",
        "    return float(np.sum(np.abs(point1 - point2)))\n",
        "\n",
        "def minkowski_distance(point1, point2, p=2):\n",
        "    return float(np.power(np.sum(np.abs(point1 - point2)**p), 1.0/p))\n",
        "\n",
        "class CustomKNN:\n",
        "    def __init__(self, k=5, distance_metric='euclidean', weights='uniform', minkowski_p=2):\n",
        "        self.k = int(k)\n",
        "        self.distance_metric = distance_metric\n",
        "        self.weights = weights\n",
        "        self.minkowski_p = minkowski_p\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.asarray(X, dtype=float)\n",
        "        self.y_train = np.asarray(y, dtype=float)\n",
        "        return self\n",
        "\n",
        "    def _calculate_distance(self, p1, p2):\n",
        "        if self.distance_metric == 'euclidean':\n",
        "            return euclidean_distance(p1, p2)\n",
        "        if self.distance_metric == 'manhattan':\n",
        "            return manhattan_distance(p1, p2)\n",
        "        if self.distance_metric == 'minkowski':\n",
        "            return minkowski_distance(p1, p2, p=self.minkowski_p)\n",
        "        raise ValueError(\"Unsupported distance_metric\")\n",
        "\n",
        "    def _get_neighbors(self, test_point):\n",
        "        dists = np.array([self._calculate_distance(test_point, tr) for tr in self.X_train])\n",
        "        idx = np.argsort(dists)[:self.k]\n",
        "        return idx, dists[idx]\n",
        "\n",
        "    def predict_single(self, test_point):\n",
        "        idx, d = self._get_neighbors(test_point)\n",
        "        y_neighbors = self.y_train[idx]\n",
        "        if self.weights == 'uniform':\n",
        "            return float(np.mean(y_neighbors))\n",
        "        if self.weights == 'distance':\n",
        "            if np.any(d == 0):\n",
        "                return float(y_neighbors[d == 0][0])\n",
        "            w = 1.0 / d\n",
        "            return float(np.sum(w * y_neighbors) / np.sum(w))\n",
        "        raise ValueError(\"Unsupported weights\")\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return np.array([self.predict_single(x) for x in X_test])\n",
        "\n",
        "    def score(self, X_test, y_test):\n",
        "        y_pred = self.predict(X_test)\n",
        "        return r2_score(y_test, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "id": "R12Gp2gLtjyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation Decisions\n",
        "- **Alternatives I thought about:** KD-Tree or Ball-Tree for faster neighbor searches, but I stuck with brute force to focus on the fundamentals.  \n",
        "- **Limitations:** My custom k-NN is slower on large datasets, and the feature “importance” measure I added is just an approximation.  \n",
        "- **Scaling choice:** StandardScaler helped keep things robust in the presence of outliers.\n",
        "\n",
        "---\n",
        "\n",
        "## Personal Reflection\n",
        "The hardest part for me understanding the k-NN class coding and debugging the distance-weighted prediction. Once it worked, I felt like I understood the algorithm a lot better.\n",
        "\n",
        "If I had more time, I’d add KD-Tree optimization and try other weighting schemes beyond inverse distance.\n",
        "\n",
        "For real-world use cases, k-NN could be applied in recommendation systems (finding similar users or movies), anomaly detection in financial data, and regression problems like property valuation.\n",
        "\n"
      ],
      "metadata": {
        "id": "H9Pp5r0xtxyV"
      }
    }
  ]
}