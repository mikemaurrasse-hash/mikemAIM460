{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Comprehensive k-NN Assignment \u2014 Real Estate Price Prediction\n**Student:** Mike Maurrasse  \n**Filename:** `maurrasse_mike_knn_assignment.ipynb`  \n**Date:** 2025-08-31\n\n> Full pipeline with custom k-NN, EDA, preprocessing, manual calculations, tuning, and analysis. All plots use matplotlib."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 0 \u2014 Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Basic imports\nimport os, math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.datasets import fetch_california_housing\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 120)\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"Environment initialized.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 1 \u2014 Data Loading and Exploration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# 1.1 Data Loading and Initial Inspection\n# First, try to fetch California Housing. If that fails, fall back to local CSV.\nimport pandas as pd\n\ntry:\n    from sklearn.datasets import fetch_california_housing\n    cali = fetch_california_housing(as_frame=True)\n    df = cali.frame.copy()\n    df.columns = ['MedInc','HouseAge','AveRooms','AveBedrms','Population','AveOccup','Latitude','Longitude','MedHouseVal']\n    target_col = 'MedHouseVal'\n    print(\"Loaded California Housing dataset from sklearn.\")\nexcept Exception as e:\n    print(\"Fetching California Housing failed. Using synthetic_california_housing.csv fallback. Error:\", e)\n    df = pd.read_csv(\"synthetic_california_housing.csv\")\n    target_col = \"MedHouseVal\"\n\nprint(\"Shape:\", df.shape)\ndisplay(df.head())\ndisplay(df.tail())\ndisplay(df.info())\ndisplay(df.describe().T)\nprint(\"Target variable:\", target_col)\nnumeric_features = [c for c in df.columns if c != target_col]\nprint(\"Numeric features:\", numeric_features)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 1.2 Comprehensive EDA"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### A. Target Variable Analysis"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Target distribution\nfig = plt.figure()\nplt.hist(df[target_col], bins=40)\nplt.title(\"Target Distribution: \" + target_col)\nplt.xlabel(target_col); plt.ylabel(\"Count\")\nplt.show()\n\n# Boxplot\nfig = plt.figure()\nplt.boxplot(df[target_col], vert=True)\nplt.title(\"Target Boxplot: \" + target_col)\nplt.ylabel(target_col)\nplt.show()\n\n# Summary + simple IQR outlier estimate\nprint(\"Summary stats for target:\")\ndisplay(df[target_col].describe())\nq1, q3 = df[target_col].quantile([0.25, 0.75])\niqr = q3 - q1\nlower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\nprint(\"IQR bounds:\", lower, upper)\nprint(\"Potential outliers (count):\", ((df[target_col] < lower) | (df[target_col] > upper)).sum())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### B. Feature Analysis"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Histograms for features\nfor c in numeric_features:\n    fig = plt.figure()\n    plt.hist(df[c], bins=40)\n    plt.title(f\"Feature Distribution: {c}\")\n    plt.xlabel(c); plt.ylabel(\"Count\")\n    plt.show()\n\n# Correlation heatmap\ncorr = df[numeric_features + [target_col]].corr()\nfig = plt.figure()\nplt.imshow(corr, interpolation='nearest')\nplt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\nplt.yticks(range(len(corr.columns)), corr.columns)\nplt.colorbar()\nplt.title(\"Correlation Matrix\")\nplt.tight_layout()\nplt.show()\n\n# Scatter vs target\nfor c in numeric_features:\n    fig = plt.figure()\n    plt.scatter(df[c], df[target_col], s=6)\n    plt.xlabel(c); plt.ylabel(target_col)\n    plt.title(f\"{c} vs {target_col}\")\n    plt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### C. Geographic Analysis"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nfig = plt.figure()\nplt.scatter(df['Longitude'], df['Latitude'], c=df[target_col], s=6)\nplt.xlabel(\"Longitude\"); plt.ylabel(\"Latitude\")\nplt.title(\"Geography: Price colored by target\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "#### D. Feature Relationships"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ncorr_target = df[numeric_features + [target_col]].corr()[target_col].drop(target_col).abs().sort_values(ascending=False)\ntop3 = list(corr_target.head(3).index)\nprint(\"Top 3 features by |corr| with target:\", top3)\n\nfor c in top3:\n    fig = plt.figure()\n    x = df[c].values; y = df[target_col].values\n    plt.scatter(x, y, s=6)\n    m, b = np.polyfit(x, y, 1)\n    xs = np.linspace(x.min(), x.max(), 120)\n    ys = m*xs + b\n    plt.plot(xs, ys)\n    plt.xlabel(c); plt.ylabel(target_col)\n    plt.title(f\"{c} vs {target_col} with trend line\")\n    plt.show()\n\n# Multicollinearity (top correlated pairs among features)\ncorr_no_target = df[numeric_features].corr().abs()\nupper = corr_no_target.where(np.triu(np.ones(corr_no_target.shape), k=1).astype(bool))\ntop_pairs = upper.stack().sort_values(ascending=False).head(10)\ndisplay(top_pairs)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**EDA Write-up (2\u20133 paragraphs):** Summarize target distribution and outliers; strongest feature relations; geographic patterns; expected modeling challenges (skew/outliers/multicollinearity) and scaling choices."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 2 \u2014 Data Cleaning and Preprocessing"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 Missing Value Analysis"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nmissing = df.isna().sum()\nprint(\"Missing values per column:\")\ndisplay(missing)\n# Document handling decisions here if any missing are found.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.2 Outlier Detection and Handling"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef detect_outliers_iqr(data, column, factor=1.5):\n    # IQR-based boolean mask\n    q1 = data[column].quantile(0.25)\n    q3 = data[column].quantile(0.75)\n    iqr = q3 - q1\n    lower, upper = q1 - factor*iqr, q3 + factor*iqr\n    return (data[column] < lower) | (data[column] > upper)\n\ndef detect_outliers_zscore(data, column, threshold=3.0):\n    # Z-score based boolean mask\n    vals = data[column].astype(float).values\n    mu, sigma = np.mean(vals), np.std(vals)\n    if sigma == 0:\n        return pd.Series([False]*len(vals), index=data.index)\n    z = (vals - mu) / sigma\n    return pd.Series(np.abs(z) > threshold, index=data.index)\n\niqr_flags = pd.DataFrame(index=df.index)\nz_flags = pd.DataFrame(index=df.index)\nfor c in numeric_features:\n    iqr_flags[c] = detect_outliers_iqr(df, c, factor=1.5)\n    z_flags[c] = detect_outliers_zscore(df, c, threshold=3.0)\n\nprint(\"IQR outlier counts per feature:\")\ndisplay(iqr_flags.sum())\nprint(\"Z-score outlier counts per feature:\")\ndisplay(z_flags.sum())\n\n# Example before/after hist for the most flagged feature by IQR\nfocus_feat = iqr_flags.sum().sort_values(ascending=False).index[0]\nfig = plt.figure(); plt.hist(df[focus_feat], bins=40); plt.title(f\"Before: {focus_feat}\"); plt.show()\n\nclean_df = df.copy()\nmask_remove = iqr_flags[focus_feat] & z_flags[focus_feat]\nclean_df = clean_df.loc[~mask_remove].reset_index(drop=True)\n\nfig = plt.figure(); plt.hist(clean_df[focus_feat], bins=40); plt.title(f\"After (example): {focus_feat}\"); plt.show()\n\nprint(\"Impact on dataset size (example policy):\", df.shape, \"->\", clean_df.shape)\nfinal_df = df.copy()  # keep full data by default; justify your final policy in write-up.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.3 Feature Engineering"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef haversine(lat1, lon1, lat2, lon2):\n    # Haversine distance in kilometers\n    R = 6371.0\n    p1, p2 = np.radians(lat1), np.radians(lat2)\n    dphi = np.radians(lat2 - lat1)\n    dl = np.radians(lon2 - lon1)\n    a = np.sin(dphi/2)**2 + np.cos(p1)*np.cos(p2)*np.sin(dl/2)**2\n    return 2*R*np.arcsin(np.sqrt(a))\n\naug = final_df.copy()\naug['rooms_per_household'] = aug['AveRooms'] / aug['AveOccup']\naug['bedrooms_per_room'] = aug['AveBedrms'] / aug['AveRooms']\naug['population_per_household'] = aug['Population'] / aug['AveOccup']\n\naug['distance_to_LA'] = haversine(aug['Latitude'], aug['Longitude'], 34.0522, -118.2437)\naug['distance_to_SF'] = haversine(aug['Latitude'], aug['Longitude'], 37.7749, -122.4194)\naug['coastal_proximity'] = (aug['Longitude'] > -121).astype(int)\n\ndef income_category(v):\n    if v < 3: return \"Low\"\n    if v < 6: return \"Medium\"\n    if v < 9: return \"High\"\n    return \"Very High\"\n\ndef house_age_category(v):\n    if v < 10: return \"New\"\n    if v < 30: return \"Medium\"\n    return \"Old\"\n\naug['income_category'] = aug['MedInc'].apply(income_category)\naug['house_age_category'] = aug['HouseAge'].apply(house_age_category)\ndisplay(aug.head())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 3 \u2014 Custom k-NN Implementation"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.1 Distance Metrics"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef euclidean_distance(point1, point2):\n    # Euclidean distance between two 1D arrays\n    diff = point1 - point2\n    return float(np.sqrt(np.dot(diff, diff)))\n\ndef manhattan_distance(point1, point2):\n    # Manhattan (L1) distance\n    return float(np.sum(np.abs(point1 - point2)))\n\ndef minkowski_distance(point1, point2, p=2):\n    # Minkowski distance of order p\n    return float(np.power(np.sum(np.abs(point1 - point2)**p), 1.0/p))\n\n# Quick checks\na = np.array([0.,0.]); b = np.array([3.,4.])\nprint(\"Euclidean((0,0),(3,4)) =\", euclidean_distance(a,b))\nprint(\"Manhattan((0,0),(3,4)) =\", manhattan_distance(a,b))\nprint(\"Minkowski p=3 =\", minkowski_distance(a,b,p=3))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 3.2 k-NN Class"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nclass CustomKNN:\n    # Custom k-NN regressor supporting Euclidean, Manhattan, or Minkowski distance and uniform or distance weights.\n    def __init__(self, k=5, distance_metric='euclidean', weights='uniform', minkowski_p=2):\n        self.k = int(k)\n        self.distance_metric = distance_metric\n        self.weights = weights\n        self.minkowski_p = minkowski_p\n        self.X_train = None\n        self.y_train = None\n\n    def fit(self, X, y):\n        # Store training data\n        self.X_train = np.asarray(X, dtype=float)\n        self.y_train = np.asarray(y, dtype=float)\n        if self.X_train.ndim != 2: raise ValueError(\"X must be 2D\")\n        if self.y_train.ndim != 1: raise ValueError(\"y must be 1D\")\n        if len(self.X_train) != len(self.y_train): raise ValueError(\"X and y length mismatch\")\n        return self\n\n    def _calculate_distance(self, p1, p2):\n        # Compute distance according to selected metric\n        if self.distance_metric == 'euclidean':\n            return euclidean_distance(p1, p2)\n        if self.distance_metric == 'manhattan':\n            return manhattan_distance(p1, p2)\n        if self.distance_metric == 'minkowski':\n            return minkowski_distance(p1, p2, p=self.minkowski_p)\n        raise ValueError(\"Unsupported distance_metric\")\n\n    def _get_neighbors(self, test_point):\n        # Return indices and distances of k nearest neighbors\n        dists = np.array([self._calculate_distance(test_point, tr) for tr in self.X_train])\n        idx = np.argsort(dists)[:self.k]\n        return idx, dists[idx]\n\n    def predict_single(self, test_point):\n        # Predict for one point (uniform or distance weights)\n        idx, d = self._get_neighbors(test_point)\n        y_neighbors = self.y_train[idx]\n        if self.weights == 'uniform':\n            return float(np.mean(y_neighbors))\n        if self.weights == 'distance':\n            if np.any(d == 0):  # exact match\n                return float(y_neighbors[d == 0][0])\n            w = 1.0 / d\n            return float(np.sum(w * y_neighbors) / np.sum(w))\n        raise ValueError(\"Unsupported weights\")\n\n    def predict(self, X_test):\n        X_test = np.asarray(X_test, dtype=float)\n        return np.array([self.predict_single(x) for x in X_test])\n\n    def score(self, X_test, y_test):\n        y_pred = self.predict(X_test)\n        return r2_score(y_test, y_pred)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 4 \u2014 Manual Calculations (Proof of Understanding)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4.1 Distance Calculations \u2014 Fill in by hand and verify below"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# After building X later, select three rows by index and print step-by-step math.\ndef print_manual_distance_steps(p1, p2, p3, feature_names):\n    def fmt(v): return \"[\" + \", \".join(f\"{x:.4f}\" for x in v) + \"]\"\n    print(\"Point1:\", fmt(p1)); print(\"Point2:\", fmt(p2)); print(\"Point3:\", fmt(p3))\n    # Euclidean P1-P2\n    dif12 = p1 - p2\n    print(\"\\nEuclidean(P1,P2) = sqrt(sum (dif^2))\")\n    print(\"Diffs:\", fmt(dif12))\n    print(\"Squares:\", fmt(dif12**2))\n    print(\"Sum squares:\", float(np.sum(dif12**2)))\n    print(\"Euclidean =\", float(np.sqrt(np.sum(dif12**2))), \" | verify:\", euclidean_distance(p1,p2))\n    # Manhattan P1-P3\n    dif13 = np.abs(p1 - p3)\n    print(\"\\nManhattan(P1,P3) = sum |dif|\")\n    print(\"Abs diffs:\", fmt(dif13))\n    print(\"Sum abs diffs:\", float(np.sum(dif13)))\n    print(\"Verify:\", manhattan_distance(p1,p3))\n    # Minkowski p=3 P2-P3\n    dif23 = np.abs(p2 - p3)\n    print(\"\\nMinkowski p=3 (P2,P3) = (sum |dif|^3)^(1/3)\")\n    print(\"Abs diffs:\", fmt(dif23))\n    print(\"Cubes:\", fmt(dif23**3))\n    print(\"Sum cubes:\", float(np.sum(dif23**3)))\n    print(\"Minkowski p=3 =\", float(np.power(np.sum(dif23**3), 1/3)), \" | verify:\", minkowski_distance(p2,p3,p=3))\n\nprint(\"After feature matrix is built, select 3 indices and call print_manual_distance_steps(...) to include in your PDF.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 4.2 k-NN Prediction Walkthrough \u2014 Manual Neighbor Finding"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef manual_knn_demo(X_train, y_train, test_point, k=5, metric='euclidean'):\n    def d(a,b):\n        if metric == 'euclidean': return euclidean_distance(a,b)\n        if metric == 'manhattan': return manhattan_distance(a,b)\n        return minkowski_distance(a,b,p=2)\n    dists = np.array([d(test_point, tr) for tr in X_train])\n    order = np.argsort(dists)\n    print(\"First 10 distances (idx, dist, y):\")\n    for i in range(10):\n        idx = order[i]\n        print(f\"{i+1:2d}. idx={idx}, dist={dists[idx]:.6f}, y={y_train[idx]:.4f}\")\n    k_idx = order[:k]\n    k_d = dists[k_idx]\n    k_y = y_train[k_idx]\n    print(\"\\n5 Nearest Neighbors:\")\n    for r,(idx,di,yi) in enumerate(zip(k_idx,k_d,k_y), start=1):\n        print(f\"{r}. idx={idx}, dist={di:.6f}, target={yi:.4f}\")\n    # Uniform vs distance-weighted\n    uniform_pred = float(np.mean(k_y))\n    if np.any(k_d == 0): distance_pred = float(k_y[k_d==0][0])\n    else:\n        w = 1.0/k_d\n        distance_pred = float(np.sum(w*k_y)/np.sum(w))\n    print(\"\\nUniform Prediction:\", uniform_pred)\n    print(\"Distance-weighted Prediction:\", distance_pred)\n    return uniform_pred, distance_pred\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 5 \u2014 Model Evaluation and Hyperparameter Tuning"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5.1 Train\u2013Test Split and Scaling"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Build modeling matrix with engineered features and one-hot categories\nX_base = aug.drop(columns=[target_col]).copy()\n# One-hot encode categorical features\ncat_df = pd.get_dummies(X_base[['income_category','house_age_category']], drop_first=True)\nX_num = X_base.drop(columns=['income_category','house_age_category'])\nX = pd.concat([X_num, cat_df], axis=1)\ny = final_df[target_col].values.astype(float)\n\nX_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0.2, random_state=RANDOM_STATE)\n\nscalers = {\"standard\": StandardScaler(), \"minmax\": MinMaxScaler()}\nscaled_data = {}\nfor name, sc in scalers.items():\n    sc_fit = sc.fit(X_train)\n    scaled_data[name] = (sc_fit, sc_fit.transform(X_train), sc_fit.transform(X_test))\nprint(\"Prepared StandardScaler and MinMaxScaler versions. Justify your scaling choice in write-up.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5.2 Manual Grid Search (5-fold CV)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef manual_grid_search(X_train, y_train, X_val, y_val, param_grid):\n    results = []\n    kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n    for k in param_grid.get('k', [5]):\n        for dm in param_grid.get('distance_metric', ['euclidean']):\n            for w in param_grid.get('weights', ['uniform']):\n                # CV\n                cv_scores = []\n                for tr_idx, va_idx in kf.split(X_train):\n                    Xtr, Xva = X_train[tr_idx], X_train[va_idx]\n                    ytr, yva = y_train[tr_idx], y_train[va_idx]\n                    model = CustomKNN(k=k, distance_metric=dm, weights=w)\n                    model.fit(Xtr, ytr)\n                    yva_pred = model.predict(Xva)\n                    cv_scores.append(r2_score(yva, yva_pred))\n                cv_mean = float(np.mean(cv_scores))\n                # Hold-out validation\n                model = CustomKNN(k=k, distance_metric=dm, weights=w)\n                model.fit(X_train, y_train)\n                val_score = float(r2_score(y_val, model.predict(X_val)))\n                results.append({'k':k, 'distance_metric':dm, 'weights':w, 'cv_mean_r2':cv_mean, 'val_r2':val_score})\n    res_df = pd.DataFrame(results).sort_values(by=['val_r2','cv_mean_r2'], ascending=False)\n    best = res_df.iloc[0].to_dict()\n    best_params = {'k': int(best['k']), 'distance_metric': best['distance_metric'], 'weights': best['weights']}\n    return best_params, res_df\n\n# Use StandardScaler set for tuning\nscaler, Xtr_s, Xte_s = scaled_data['standard']\nXtr_g, Xval_g, ytr_g, yval_g = train_test_split(Xtr_s, y_train, test_size=0.2, random_state=RANDOM_STATE)\n\nparam_grid = {'k':[3,5,7,9,11,15,21], 'distance_metric':['euclidean','manhattan'], 'weights':['uniform','distance']}\nbest_params, results_df = manual_grid_search(Xtr_g, ytr_g, Xval_g, yval_g, param_grid)\nprint(\"Best params:\", best_params)\ndisplay(results_df.head(10))\n\n# Simple visualization: CV mean vs k\nfig = plt.figure()\nplt.plot(results_df['k'], results_df['cv_mean_r2'], 'o')\nplt.xlabel(\"k\"); plt.ylabel(\"CV mean R^2\"); plt.title(\"CV mean R^2 vs k (all combos)\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 5.3 Performance Analysis"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Final model and metrics\nfinal_model = CustomKNN(**best_params)\nfinal_model.fit(Xtr_s, y_train)\ny_pred_test = final_model.predict(Xte_s)\nprint(\"Final Test R^2:\", r2_score(y_test, y_pred_test))\nprint(\"Test RMSE:\", mean_squared_error(y_test, y_pred_test, squared=False))\n\n# Learning curves vs k (using test as generalization proxy here)\nk_values = [3,5,7,9,11,15,21]\ntrain_scores, test_scores = [], []\nfor k in k_values:\n    m = CustomKNN(k=k, distance_metric=best_params['distance_metric'], weights=best_params['weights'])\n    m.fit(Xtr_s, y_train)\n    train_scores.append(r2_score(y_train, m.predict(Xtr_s)))\n    test_scores.append(r2_score(y_test, m.predict(Xte_s)))\n\nfig = plt.figure()\nplt.plot(k_values, train_scores, marker='o', label='Train R^2')\nplt.plot(k_values, test_scores, marker='o', label='Test R^2')\nplt.xlabel(\"k\"); plt.ylabel(\"R^2\"); plt.title(\"Learning Curve: R^2 vs k\"); plt.legend()\nplt.show()\n\n# Distance metric comparison\nfor dm in ['euclidean','manhattan']:\n    m = CustomKNN(k=best_params['k'], distance_metric=dm, weights=best_params['weights'])\n    m.fit(Xtr_s, y_train)\n    print(f\"{dm} -> Test R^2: {r2_score(y_test, m.predict(Xte_s)):.4f}\")\n\n# Residual analysis\nresid = y_test - y_pred_test\nfig = plt.figure(); plt.hist(resid, bins=40); plt.title(\"Residuals\"); plt.xlabel(\"Residual\"); plt.ylabel(\"Count\"); plt.show()\nfig = plt.figure(); plt.scatter(y_pred_test, resid, s=6); plt.axhline(0); plt.xlabel(\"Predicted\"); plt.ylabel(\"Residual\"); plt.title(\"Residuals vs Predicted\"); plt.show()\n\n# Neighbor \"feature contribution\" proxy\ndef neighbor_feature_contributions(Xref, Xpts, k=5, max_points=200):\n    n = Xref.shape[1]\n    contrib = np.zeros(n)\n    take = min(max_points, len(Xpts))\n    for i in range(take):\n        x = Xpts[i]\n        d = np.linalg.norm(Xref - x, axis=1)\n        idx = np.argsort(d)[:k]\n        diffs = np.abs(Xref[idx] - x)\n        rs = diffs.sum(axis=1, keepdims=True); rs[rs==0] = 1.0\n        frac = diffs / rs\n        contrib += frac.mean(axis=0)\n    return contrib / take\n\nfeat_contrib = neighbor_feature_contributions(Xtr_s, Xte_s, k=best_params['k'])\ncontrib_series = pd.Series(feat_contrib, index=list(X.columns)).sort_values(ascending=False)\ndisplay(contrib_series.head(15))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Part 6 \u2014 Comparison and Advanced Analysis"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6.1 Sklearn Comparison"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Compare custom vs sklearn\nmetric = 'minkowski' if best_params['distance_metric']=='euclidean' else best_params['distance_metric']\np = 2 if best_params['distance_metric']=='euclidean' else 1\nsk = KNeighborsRegressor(n_neighbors=best_params['k'], metric=metric, p=p, weights=best_params['weights'])\nsk.fit(Xtr_s, y_train)\nsk_pred = sk.predict(Xte_s)\nprint(\"CustomKNN Test R^2:\", r2_score(y_test, y_pred_test))\nprint(\"Sklearn KNN Test R^2:\", r2_score(y_test, sk_pred))\nprint(\"Absolute R^2 diff:\", abs(r2_score(y_test, y_pred_test) - r2_score(y_test, sk_pred)))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 6.2 Curse of Dimensionality"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nfrom sklearn.datasets import make_regression\n\ndims = [2,4,8,16,32,64]\nmse_list = []\nfor d in dims:\n    Xsyn, ysyn = make_regression(n_samples=2000, n_features=d, noise=10.0, random_state=RANDOM_STATE)\n    Xtr, Xte, ytr, yte = train_test_split(Xsyn, ysyn, test_size=0.2, random_state=RANDOM_STATE)\n    sc = StandardScaler().fit(Xtr)\n    Xtr_s = sc.transform(Xtr); Xte_s = sc.transform(Xte)\n    knn = KNeighborsRegressor(n_neighbors=7, metric='euclidean', weights='distance')\n    knn.fit(Xtr_s, ytr)\n    pred = knn.predict(Xte_s)\n    mse_list.append(mean_squared_error(yte, pred))\n    print(f\"d={d:2d} -> Test MSE: {mse_list[-1]:.2f}\")\n\nfig = plt.figure()\nplt.plot(dims, mse_list, marker='o')\nplt.xlabel(\"Dimensions\"); plt.ylabel(\"Test MSE\"); plt.title(\"Curse of Dimensionality for k-NN\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Conceptual Questions \u2014 Answer in your own words"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1) Why might Manhattan distance be preferable to Euclidean distance in certain scenarios?\n\n2) How does k affect the bias\u2013variance tradeoff in k-NN?\n\n3) Computational implications of different distance metrics?\n\n4) How would you modify k-NN for categorical features?"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Implementation Decisions"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Alternatives considered; limitations; scaling choice justification."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Personal Reflection"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Most challenging part; improvements with more time; real-world applications that benefit from this analysis."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}